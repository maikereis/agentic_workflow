{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ollama import Client\n",
    "from IPython.display import display_markdown, display_latex\n",
    "\n",
    "from src.tool import ToolKit, ToolCallProcessor\n",
    "from functions import summation, multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to local Ollama_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "model_name = \"qwen2.5:7b\"\n",
    "\n",
    "# Pull a model\n",
    "result = client.pull(model=model_name)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registry functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o registro de ferramentas\n",
    "toolkit = ToolKit(tools=[summation, multiplication])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = \"\\n\\n\".join(\n",
    "    [json.dumps(schema, indent=4) for schema in toolkit.tools_schemas()]\n",
    ")\n",
    "\n",
    "\n",
    "TOOL_PROMPT = (\n",
    "    \"\"\"\n",
    "You are a function calling AI model.\n",
    "If a function or tool is unavailable, responde with trained data.\n",
    "You may call one or more functions to assist with the user query.\n",
    "You are provided with function signatures within <tools></tools> XML tags, here are the available tools:\n",
    "<tools>\n",
    "%s\n",
    "</tools>\n",
    "\n",
    "For each function call, to a listed function, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-json-object>, \"id\": <monotonically-increasing-id>}\n",
    "</tool_call>\n",
    "\"\"\"\n",
    "    % tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": TOOL_PROMPT,\n",
    "    }\n",
    "]\n",
    "\n",
    "agent_chat_history = []\n",
    "\n",
    "USER_PROMPT = \"Can you multiple 25 by 12 and sum 3 with 6 and search what is an LLM on google.com?\"\n",
    "\n",
    "tool_chat_history.append({\"role\": \"user\", \"content\": USER_PROMPT})\n",
    "agent_chat_history.append({\"role\": \"user\", \"content\": USER_PROMPT})\n",
    "\n",
    "# Request tools action\n",
    "tools_calls_request = client.chat(model=model_name, messages=tool_chat_history)\n",
    "# Parse the response into model default format\n",
    "tools_calls_str = tools_calls_request[\"message\"][\"content\"]\n",
    "\n",
    "tool_calls = ToolCallProcessor(tools_calls_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.tool._base.ToolCallProcessor at 0x7fba9560ca60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'run'\n"
     ]
    }
   ],
   "source": [
    "# Return the final response using the tools retrivied info\n",
    "for call in tool_calls.tool_calls:\n",
    "    try:\n",
    "        result = toolkit.get_tool_by_name(call.name).run(call.arguments)\n",
    "\n",
    "        agent_chat_history.append(\n",
    "            {\"role\": \"tool\", \"content\": f\"result of call to {call.name}: {result}\"}\n",
    "        )\n",
    "    except AttributeError as ae:\n",
    "        print(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\nYou are a function calling AI model.\\nIf a function or tool is unavailable, responde with trained data.\\nYou may call one or more functions to assist with the user query.\\nYou are provided with function signatures within <tools></tools> XML tags, here are the available tools:\\n<tools>\\n{\\n    \"name\": \"summation\",\\n    \"doc\": \"\\\\n    Add two integers.\\\\n\\\\n    Parameters\\\\n    ----------\\\\n    x : int\\\\n        The first integer to be added.\\\\n    y : int, optional\\\\n        The second integer to be added (default is 0).\\\\n\\\\n    Returns\\\\n    -------\\\\n    int\\\\n        The sum of `x` and `y`.\\\\n    \",\\n    \"parameters\": {\\n        \"properties\": {\\n            \"x\": {\\n                \"type\": \"int\"\\n            },\\n            \"y\": {\\n                \"type\": \"int\"\\n            }\\n        }\\n    }\\n}\\n\\n{\\n    \"name\": \"multiplication\",\\n    \"doc\": \"\\\\n    Multiply two integers.\\\\n\\\\n    Parameters\\\\n    ----------\\\\n    x : int\\\\n        The first integer to be multiplied.\\\\n    y : int\\\\n        The second integer to be multiplied.\\\\n\\\\n    Returns\\\\n    -------\\\\n    int\\\\n        The product of `x` and `y`.\\\\n    \",\\n    \"parameters\": {\\n        \"properties\": {\\n            \"x\": {\\n                \"type\": \"int\"\\n            },\\n            \"y\": {\\n                \"type\": \"int\"\\n            }\\n        }\\n    }\\n}\\n</tools>\\n\\nFor each function call, to a listed function, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>, \"id\": <monotonically-increasing-id>}\\n</tool_call>\\n'},\n",
       " {'role': 'user',\n",
       "  'content': 'Can you multiple 25 by 12 and sum 3 with 6 and search what is an LLM on google.com?'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Can you multiple 25 by 12 and sum 3 with 6 and search what is an LLM on google.com?'},\n",
       " {'role': 'tool', 'content': 'result of call to multiplication: 300'},\n",
       " {'role': 'tool', 'content': 'result of call to summation: 9'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, let's break it down:\n",
       "\n",
       "1. Multiplying 25 by 12 gives us 300.\n",
       "2. Adding 3 and 6 together gives us a sum of 9.\n",
       "\n",
       "Now, if you're asking me to search for \"LLM\" on Google, I can provide information based on common understanding or general knowledge, but I won't actually perform an internet search in real-time. An LLM typically stands for Large Language Model, which is a type of artificial intelligence model used in natural language processing tasks such as text generation and translation.\n",
       "\n",
       "Would you like more details about LLMs or anything else?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_response = client.chat(model=model_name, messages=agent_chat_history)\n",
    "\n",
    "response_using_tool = tool_response[\"message\"][\"content\"]\n",
    "\n",
    "display_markdown(response_using_tool, raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
