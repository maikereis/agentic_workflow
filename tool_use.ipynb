{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "from IPython.display import display_markdown, display_latex\n",
    "\n",
    "from src.agentic.tool.tool import ToolRegistry, ToolCallProcessor\n",
    "from functions import summation, multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to local Ollama_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "model_name = \"qwen2.5:7b\"\n",
    "\n",
    "# Pull a model\n",
    "result = client.pull(model=model_name)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registry functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o registro de ferramentas\n",
    "tool_registry = ToolRegistry()\n",
    "\n",
    "# Registrar as funções\n",
    "tool_registry.register(\"summation\", summation)\n",
    "tool_registry.register(\"multiplication\", multiplication)\n",
    "\n",
    "tools_schemas_string = tool_registry.generate_tools_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_PROMPT = \"\"\"\n",
    "You are a function calling AI model.\n",
    "If a function or tool is unavailable, responde with trained data.\n",
    "You may call one or more functions to assist with the user query.\n",
    "You are provided with function signatures within <tools></tools> XML tags, here are the available tools:\n",
    "<tools>\n",
    "%s\n",
    "</tools>\n",
    "\n",
    "For each function call, to a listed function, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-json-object>, \"id\": <monotonically-increasing-id>}\n",
    "</tool_call>\n",
    "\"\"\" % tools_schemas_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, the multiplication of 25 by 12 equals 300, and the sum of 3 with 6 equals 9. \n",
       "\n",
       "To answer your last request: LLM stands for Large Language Model. It is a type of artificial intelligence model designed to understand and generate human-like text. These models are typically trained on large amounts of text data and can perform various language-related tasks such as translation, summarization, question answering, and more. If you'd like me to search \"what is an LLM\" for you using Google, I can provide a brief summary based on the information available, but I won't actually perform the web search itself. Would you like to proceed with that?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": TOOL_PROMPT,\n",
    "    }\n",
    "]\n",
    "\n",
    "agent_chat_history = []\n",
    "\n",
    "USER_PROMPT = \"Can you multiple 25 by 12 and sum 3 with 6 and search what is an LLM on google.com?\"\n",
    "\n",
    "tool_chat_history.append(\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    ")\n",
    "agent_chat_history.append(\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    ")\n",
    "\n",
    "# Request tools action\n",
    "tools_calls_request = client.chat(model=model_name, messages=tool_chat_history)\n",
    "\n",
    "# Parse the response into model default format\n",
    "tools_calls_str = tools_calls_request['message']['content']\n",
    "\n",
    "processor = ToolCallProcessor(tool_registry)\n",
    "output = processor.process_tool_calls(tools_calls_str)\n",
    "\n",
    "# Return the final response using the tools retrivied info\n",
    "agent_chat_history.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": output\n",
    "})\n",
    "\n",
    "tool_response = client.chat(model=model_name, messages=agent_chat_history)\n",
    "\n",
    "response_using_tool = tool_response['message']['content']\n",
    "\n",
    "display_markdown(response_using_tool, raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
