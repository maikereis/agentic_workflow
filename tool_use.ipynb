{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ollama import Client\n",
    "from IPython.display import display_markdown, display_latex\n",
    "\n",
    "from src.tool import ToolKit, ToolCallProcessor\n",
    "from functions import summation, multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to local Ollama_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "model_name = \"qwen2.5:7b\"\n",
    "\n",
    "# Pull a model\n",
    "result = client.pull(model=model_name)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registry functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o registro de ferramentas\n",
    "toolkit = ToolKit(tools=[summation, multiplication])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = \"\\n\\n\".join([json.dumps(schema, indent=4) for schema in toolkit.tools_schemas()])\n",
    "\n",
    "\n",
    "TOOL_PROMPT = \"\"\"\n",
    "You are a function calling AI model.\n",
    "If a function or tool is unavailable, responde with trained data.\n",
    "You may call one or more functions to assist with the user query.\n",
    "You are provided with function signatures within <tools></tools> XML tags, here are the available tools:\n",
    "<tools>\n",
    "%s\n",
    "</tools>\n",
    "\n",
    "For each function call, to a listed function, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-json-object>, \"id\": <monotonically-increasing-id>}\n",
    "</tool_call>\n",
    "\"\"\" % tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": TOOL_PROMPT,\n",
    "    }\n",
    "]\n",
    "\n",
    "agent_chat_history = []\n",
    "\n",
    "USER_PROMPT = \"Can you multiple 25 by 12 and sum 3 with 6 and search what is an LLM on google.com?\"\n",
    "\n",
    "tool_chat_history.append(\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    ")\n",
    "agent_chat_history.append(\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    ")\n",
    "\n",
    "# Request tools action\n",
    "tools_calls_request = client.chat(model=model_name, messages=tool_chat_history)\n",
    "# Parse the response into model default format\n",
    "tools_calls_str = tools_calls_request['message']['content']\n",
    "\n",
    "tool_calls = ToolCallProcessor(tools_calls_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the final response using the tools retrivied info\n",
    "for call in tool_calls.tool_calls:\n",
    "    try:\n",
    "        result = toolkit.get_tool_by_name(call.name).run(call.arguments)\n",
    "\n",
    "        agent_chat_history.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"result of call to {call.name}: {result}\"\n",
    "        })\n",
    "    except AttributeError as ae:\n",
    "        print(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, the result of multiplying 25 by 12 is 300, and the sum of 3 and 6 is 9.\n",
       "\n",
       "Now, regarding your request to search for what an LLM is on Google, while I can't directly perform a web search, I can tell you that LLM stands for Large Language Model. These are AI models designed to understand and generate human-like text. They're used in various applications like chatbots, content generation, and more. If you need more detailed information, I can help guide you on how to find it yourself or provide some general details about LLMs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_response = client.chat(model=model_name, messages=agent_chat_history)\n",
    "\n",
    "response_using_tool = tool_response['message']['content']\n",
    "\n",
    "display_markdown(response_using_tool, raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
